## **Don’tGetKicked — Деревья, Random Forest, GBDT, бустинги**

Кейс: данные соревнования **Don’tGetKicked** (Kaggle), задача — предсказать, станет ли покупка авто «плохой» (IsBadBuy = 1).  
В этом задании вы реализуете **свои деревья**, **Random Forest**, **GBDT**, а также сравните их с **LightGBM / CatBoost / XGBoost**.

---

## **1. Подготовка данных**

1. Скачайте данные **Don’tGetKicked**.
2. Разбейте на **train / validation / test** по полю `PurchDate`:
   - train — первая 1/3 по времени,
   - valid — средняя 1/3,
   - test — последняя 1/3.
3. Условие:
   ```text
   train.PurchDate < valid.PurchDate < test.PurchDate
   ```

- **Test не использовать** до последнего этапа.

### **Категориальные признаки**
- Использовать `LabelEncoder` или `OneHotEncoder`.
- `fit` только на **train**, затем `transform` на **valid/test**.
- Если появляются новые категории:
- использовать энкодеры из `category_encoders`
- например, **CountEncoder**.

---

## **2. Реализация своего Decision Tree (Classifier & Regressor)**

### **Классы**
- `DecisionTreeClassifier`
- критерий: **Gini impurity**
- `DecisionTreeRegressor`
- критерий: **MSE / дисперсия**

### **Общие требования**
- Методы:
- `fit`
- `predict`
- `predict_proba` (классификатор)
- Параметры:
- обязательный `max_depth`
- Создать отдельный класс `Node`, который:
- хранит объекты и таргеты узла,
- вычисляет impurity (Gini/MSE),
- содержит ссылки `left` и `right`.

### **Реализация логики**
- Реализовать функцию выбора **лучшего сплита**:
- перебор признаков и порогов,
- оценка impurity после разбиения.
- Построить полностью рабочий `DecisionTreeClassifier`.

### **Extra Randomized Tree**
- Создать вариант дерева со случайным поиском сплита:
- случайный выбор признака,
- случайный выбор порога.

---

## **3. Качество своего дерева**

- Добиться на validation:
```
Gini ≥ 0.10
```


---

## **4. Сравнение со sklearn DecisionTree**

- Обучить `sklearn.tree.DecisionTreeClassifier` на тех же данных.
- Посчитать Gini.
- Сравнить с вашей реализацией:
- Лучше ли sklearn?
- Если да — объяснить:
  - оптимизация критерия,
  - скорость перебора сплитов,
  - встроенная регуляризация,
  - обработка категориальных признаков,
  - и т.д.

---

## **5. Реализация RandomForestClassifier**

Создать собственный `RandomForestClassifier`, используя своё дерево.

### **Требования**
- Поддержка `random_state` (детерминизм).
- Механики:
- бэггинг по объектам,
- выбор случайного подмножества признаков (`max_features`) при сплите.

### **Цель**
- Достичь на validation:

```
Gini ≥ 0.15
```


---

## **6. Реализация GBDT (градиентный бустинг)**

Создать свой `GBDTClassifier`, используя собственные деревья.

### **Параметры**
- `max_depth`
- `number_of_trees`
- `max_features`

### **Модель**
- Функция потерь: **binary cross-entropy**
- Реализовать:
- вычисление градиентов логистической функции,
- поочерёдное обучение деревьев на остатках.

### **Формула предсказания** `F(x) = Σ trees_i(x)` или `F(x) = Σ learning_rate * trees_i(x)`


---

## **7. LightGBM, CatBoost, XGBoost**

### **Что нужно сделать**
- Обучить 3 модели:
  - LightGBM
  - CatBoost
  - XGBoost
- Настроить гиперпараметры:
  - глубина деревьев
  - learning_rate
  - n_estimators
  - регуляризация
  - max_features
  - subsample / colsample_bytree  
    и др.

### **Анализировать особенности**
- **CatBoost**:
  - нативная работа с категориальными признаками,
  - порядковые статистики,
  - отсутствие утечки.
- **XGBoost**:
  - режим DART (Dropout Additive Regression Trees),
  - разные бустинговые режимы.
- **LightGBM**:
  - построение деревьев leaf-wise,
  - быстрый histogram-based алгоритм.

### **Ответить на вопросы**
- Какая модель показывает лучший Gini?
- Почему именно она даёт лучший результат в этом датасете?

---

## **8. Финальная проверка на тесте**

После выбора лучшей модели:

### **Посчитать Gini на:**
- train
- validation
- test

### **Ответить:**
- Есть ли падение качества с validation → test?
- Признаки переобучения:
  - слишком высокий train Gini,
  - большой разрыв между train/valid/test.
- Считается ли модель переобученной? Почему?

---
